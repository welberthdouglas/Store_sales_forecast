{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport seaborn as sns\nimport shap\n\nfrom datetime import datetime, timedelta\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV,GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\n\npd.options.display.max_columns = None\npd.set_option('display.max_rows', None)\n\nseed = 42\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def WMAE(y_true, y_pred, df):\n    \"\"\"calculates the WMAE metric as specified by the competition\"\"\"\n    weights = df.IsHoliday.apply(lambda x: 5 if x != 0 else 1)\n    return np.round( np.sum( weights*abs( y_true - y_pred ))/( np.sum( weights )), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Loading Data\n\nGathering all the data available "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# train data\npath = '../input/walmart-recruiting-store-sales-forecasting/'\ntrain = pd.read_csv(path+'train.csv.zip')\nstores = pd.read_csv(path+'stores.csv')\nfeatures = pd.read_csv(path+'features.csv.zip')\n\n# test data\ntest = pd.read_csv(path+'test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Joining Tables"},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Filling missing dates\n\n\nSome Store Departments have no data for some weeks so we are going to input these instances with zero sales. More details about this is available in the EDA Section."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new index\ndates = train.Date.sort_values().unique()\ndepts = train.Dept.sort_values().unique()\nstrs = train.Store.sort_values().unique()\n\nfill_index = pd.MultiIndex.from_product([dates, strs, depts],\n                           names=['Date','Store','Dept'])\n\n# Creating holidays dataframe\nholidays = train.query('Store==1 & Dept==1')[['Date','IsHoliday']]\n\n\n# Filling with reindex\ntrain_new = train.set_index(['Date','Store','Dept']).Weekly_Sales.reindex(fill_index, fill_value = 0)\ntrain_new = train_new.to_frame()\ntrain_new.reset_index(inplace=True)\n\n# merging back with holidays\n\ntrain_new = train_new.merge(holidays, on='Date', how = 'left')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Merge\n\nMerging the new train dataset with Stores and Features "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_new.merge(stores, on = 'Store', how = 'left').merge(features.drop(columns=['IsHoliday']), on =['Store','Date'], how = 'left')\n\n#data.IsHoliday_x.equals(data.IsHoliday_y)  # columns are equal so one can be dropped\n\n# Saving merged data\ndata.to_csv(\"data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. EDA\n\nThe Exploratory Data analysis is done so we can familiarize ourselves with the data and gain insight and intuition about the problem. This comes in hand to understand future model behavior or eventual unexpected results. In this phase, is essential to follow up with business experts and other stakeholders.\n\n## Noteworthy points \n\n- There are departments with missing dates in between entries, it may be the case that zero sales are not being accounted for. In a real situation, it is important to understand if this is the case or not. In this notebook, we will consider that missing values are indicatives of zero sales; therefore, this information might be useful for the model. We will input zero sales where there are missing department data.\n- We found some negative Weekly Sales in the database. Again, it is important to understand, with a business expert or someone with experience with these data, which situations may lead to this. For this notebook, we will consider negative sales as refunds, and since the challenge asks for Sales prediction, we will replace negative values with 0. This might not be a big problem since negative values represent less then 0.4% of instance entries, but still, it is important to understand if the model is expected to predict negative values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe\ndata.describe().T\n\n# negative values for Sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Types\nprint(data.head(),'\\n\\n',data.dtypes) \n\n# datatypes are all good but cam be optimized to reduce memory usage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for Nulls\nprint(data.count(),'\\n\\n',data.isna().sum()) \n\n# null values only on markdown columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stores entries\n# data['Store'].value_counts()  \n\n# some stores have slightly more entries than others...\n# this was fixed using imputation ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(['Store','Dept']).agg(dates = ('Date', 'count'))\n\n# all departments now have the same number of entries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analisis Ideas\n\n# Total Sales by department\n# Total Sales by store\n# Pairplot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Feature Engineering\n\n- OK - Parse Dates to label encoding (months and weeks of the year)\n- OK - Encode variable \"Type\" (one hot)\n- Create variables #weeks before holiday\n- OK - Input missing Dept instances as zero sales (treating missing data as zero sales)\n- OK - What are negative sales? (1285 entries in the test database  = 0.3% max of -4988.94 and total of -88161.56 adding up to -0,000013086% of the total sales) replace with zero?\n- OK - Fill Nulls in markdown variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing Negative Sales with 0\ndata[data.Weekly_Sales < 0 ].Weekly_Sales = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FillNA markdown variables with -9999\ndata.fillna(-9999,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OneHot encoding variable store type\ndata = pd.get_dummies(data,columns=['Type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parse data into month and weekofyear columns\ndata['Month'] = data.Date.apply(lambda x : datetime.strptime(str(x),'%Y-%m-%d').month)\ndata['WeekofYear'] = data.Date.apply(lambda x : datetime.strptime(str(x),'%Y-%m-%d').isocalendar()[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# holidays weeks\ndata.query('IsHoliday == True').WeekofYear.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['IsHoliday'] = data.query('IsHoliday == True').WeekofYear\ndata['IsHoliday_1'] = data.query('WeekofYear in (5, 35, 46, 51)').WeekofYear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([pd.get_dummies(data,columns=['IsHoliday','IsHoliday_1'],prefix=['Holiday','Week_Before_Holiday']),data['IsHoliday']], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reducing memory usage\n\nprint(f'data memory usage : {data.memory_usage(deep = True).sum() / 1024**2} Mb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ints = data.select_dtypes(include='int').columns.values.tolist()\nfloats = data.select_dtypes(include='float').columns.values.tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ints:\n    data[i] = pd.to_numeric(data[i],downcast = 'integer')\n    \nfor i in floats:\n    data[i] = pd.to_numeric(data[i],downcast = 'float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'data memory usage after optimizing dtypes : {data.memory_usage(deep = True).sum() / 1024**2} Mb')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Modeling\n\nIt is good practice to start with simpler models and gradually build up complexity. Simpler models are easier to interpret and can give good insight about the importance and influence of variables in the final result. Also, it is good to have an initial model ready and running as soon as possible so we can get a baseline of performance for the next iterations. In this Notebook, however, I will skip this phase since, as shown in previous sections, the dependent variable is highly non-linear. This means that simpler models such as linear regression will perform poorly and probably won't be of much use for interpretability.\n\nI choose to jump right into a random forest model, mainly because this algorithm can deal with non-linear data with ease. Random Forests also can handle data without standardizing or normalizing, and we don't have to worry about the imputation of cyclical time variables such as month and week of the year. This simplifies the preprocessing and saves us some time, also the model will give us a pretty good approximation of an \"optimum\" model right away.\n\n## Noteworthy points\n- Out of time validation set\n- Started with a sample in the randon search so we can make some initial parameters tunning\n- Make sure of not going too deep with the trees to avoid overfiting, oot set evaluation also helps\n- Variable importances\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.fillna(0,inplace = True)\n\n# out of time validation set\noot_data = data.query(\"Date >= '2012-07-01'\")\n\noot_X = oot_data.drop(columns = ['Date','Weekly_Sales'])\noot_y = oot_data.Weekly_Sales\n\n# in time validation set\nit_data = data.query(\"Date < '2012-07-01'\")\n\ndata_sample = it_data.sample(frac=.5, random_state = seed)\n\nX = data_sample.drop(columns = ['Date','Weekly_Sales'])\ny = data_sample.Weekly_Sales\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,y_train.shape,X_test.shape,oot_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(random_state=seed, n_jobs=-1)\nrandom_grid = {'bootstrap': [True, False],\n               'max_depth': [5, 10, 20, 30, 40, 50, 60],\n               'max_features': ['auto','sqrt'],\n               'min_samples_leaf': [1, 2, 4],\n               'min_samples_split': [2, 5, 10],\n               'n_estimators': [200, 400, 600]}\n\n\nrandom_grid = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3,verbose=2, random_state=seed)\n\n# Fit the random search model\nrandom_grid.fit(X_train, y_train)\n\nprint(random_grid.best_params_)\n\nrand_reg = random_grid.best_estimator_.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_rf = pd.DataFrame(rand_reg.feature_importances_).transpose()\nimp_rf.columns = X_train.columns\nimp_rf = imp_rf.transpose().sort_values(0)\nimp_rf.columns = ['rf_importance']\n\nimp_rf.plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fine tunning\n\n# {'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 40, 'bootstrap': True}\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=40, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=500, n_jobs=None, oob_score=False,\n                      random_state=None, verbose=0, warm_start=False)\nrand_reg = rf.fit(X_train.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','Holiday_36.0','Week_Before_Holiday_35.0']),y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train MAE: {:.2f}\".format(mean_absolute_error(y_train,rand_reg.predict(X_train.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])))))\nprint(\"test MAE: {:.2f}\".format(mean_absolute_error(y_test,rand_reg.predict(X_test.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])))))\nprint(\"OOT_test MAE: {:.2f}\".format(mean_absolute_error(oot_y,rand_reg.predict(oot_X.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])))))\nprint(\"train WMAE: {:.2f}\".format(WMAE(y_train,rand_reg.predict(X_train.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])), X_train)))\nprint(\"test WMAE: {:.2f}\".format(WMAE(y_test,rand_reg.predict(X_test.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])), X_test)))\nprint(\"OOT_test WMAE: {:.2f}\".format(WMAE(oot_y,rand_reg.predict(oot_X.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])), oot_X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train MAE: {:.2f}\".format(mean_absolute_error(y_train,rand_reg.predict(X_train.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','Holiday_36.0','Week_Before_Holiday_35.0'])))))\nprint(\"test MAE: {:.2f}\".format(mean_absolute_error(y_test,rand_reg.predict(X_test.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','Holiday_36.0','Week_Before_Holiday_35.0'])))))\nprint(\"OOT_test MAE: {:.2f}\".format(mean_absolute_error(oot_y,rand_reg.predict(oot_X.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','Holiday_36.0','Week_Before_Holiday_35.0'])))))\nprint(\"train WMAE: {:.2f}\".format(WMAE(y_train,rand_reg.predict(X_train.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','Holiday_36.0','Week_Before_Holiday_35.0'])), X_train)))\nprint(\"test WMAE: {:.2f}\".format(WMAE(y_test,rand_reg.predict(X_test.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','Holiday_36.0','Week_Before_Holiday_35.0'])), X_test)))\nprint(\"OOT_test WMAE: {:.2f}\".format(WMAE(oot_y,rand_reg.predict(oot_X.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','Holiday_36.0','Week_Before_Holiday_35.0'])), oot_X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','Holiday_36.0','Week_Before_Holiday_35.0']).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(rand_reg, approximate = True)\nrf_shap_values = explainer.shap_values(X_test.iloc[0:1000,:])\nshap.summary_plot(rf_shap_values, X_test.iloc[0:1000,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Validation"},{"metadata":{},"cell_type":"markdown","source":"# 9. Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = test.merge(stores, on = 'Store', how = 'left').merge(features.drop(columns=['IsHoliday']), on =['Store','Date'], how = 'left')\n\n# FillNA markdown variables with -9999\ndata.fillna(-9999,inplace = True)\n\n# OneHot encoding variable store type\ndata = pd.get_dummies(data,columns=['Type'])\n\n# parse data into month and weekofyear columns\ndata['Month'] = data.Date.apply(lambda x : datetime.strptime(str(x),'%Y-%m-%d').month)\ndata['WeekofYear'] = data.Date.apply(lambda x : datetime.strptime(str(x),'%Y-%m-%d').isocalendar()[1])\n\n# holidays weeks\ndata.query('IsHoliday == True').WeekofYear.unique()\n\ndata['IsHoliday'] = data.query('IsHoliday == True').WeekofYear\ndata['IsHoliday_1'] = data.query('WeekofYear in (5, 35, 46, 51)').WeekofYear\n\ndata = pd.concat([pd.get_dummies(data,columns=['IsHoliday','IsHoliday_1'],prefix=['Holiday','Week_Before_Holiday']),data['IsHoliday']], axis = 1)\n\nints = data.select_dtypes(include='int').columns.values.tolist()\nfloats = data.select_dtypes(include='float').columns.values.tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','Holiday_36.0','Week_Before_Holiday_35.0']).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.fillna(0,inplace = True)\ndata['Weekly_Sales'] = rand_reg.predict(data.drop(columns = ['Date','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = data\nfinal['id'] = data['Store'].map(str) + '_' + data['Dept'].map(str) + '_' + data['Date'].map(str)\nfinal['Weekly_Sales'] = data['Weekly_Sales']\n\nfinal = final[['id','Weekly_Sales']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}